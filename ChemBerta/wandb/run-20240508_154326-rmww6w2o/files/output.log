/home/stanlo229/anaconda3/envs/MolFoundation/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/home/stanlo229/anaconda3/envs/MolFoundation/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/stanlo229/Research/Repos/MolFoundation/t5-chem/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
Epoch 0
num_of_examples 1 loss: 0.0003139112144708633 %_data_trained : 0.0
num_of_examples 401 loss: 0.006800820399075746 %_data_trained : 0.3734548306382343
num_of_examples 801 loss: 0.005494950748980046 %_data_trained : 0.7469096612764686
num_of_examples 1201 loss: 0.0042804292682558295 %_data_trained : 1.1203644919147029
num_of_examples 1601 loss: 0.003152274680323899 %_data_trained : 1.4938193225529373
num_of_examples 2001 loss: 0.0023660442465916276 %_data_trained : 1.8672741531911716
num_of_examples 2401 loss: 0.001713738339021802 %_data_trained : 2.2407289838294058
num_of_examples 2801 loss: 0.0015035840519703925 %_data_trained : 2.61418381446764
num_of_examples 3201 loss: 0.001507999775931239 %_data_trained : 2.9876386451058745
num_of_examples 3601 loss: 0.0012791312648914755 %_data_trained : 3.3610934757441084
num_of_examples 4001 loss: 0.0008925938222091645 %_data_trained : 3.7345483063823433
num_of_examples 4401 loss: 0.0007879605342168361 %_data_trained : 4.108003137020577
num_of_examples 4801 loss: 0.0006223830231465399 %_data_trained : 4.4814579676588115
num_of_examples 5201 loss: 0.0005098999908659608 %_data_trained : 4.854912798297046
num_of_examples 5601 loss: 0.00038652654504403475 %_data_trained : 5.22836762893528
num_of_examples 6001 loss: 0.0003308499464765191 %_data_trained : 5.601822459573515
num_of_examples 6401 loss: 0.00020463949622353538 %_data_trained : 5.975277290211749
num_of_examples 6801 loss: 0.000204387606063392 %_data_trained : 6.3487321208499825
num_of_examples 7201 loss: 0.0001642808661563322 %_data_trained : 6.722186951488217
num_of_examples 7601 loss: 0.00016107628267491235 %_data_trained : 7.095641782126452
num_of_examples 8001 loss: 0.00016380159271648154 %_data_trained : 7.4690966127646865
num_of_examples 8401 loss: 0.0001655402360484004 %_data_trained : 7.84255144340292
num_of_examples 8801 loss: 0.00014593366504414007 %_data_trained : 8.216006274041154
num_of_examples 9201 loss: 0.00014012476400239394 %_data_trained : 8.589461104679389
num_of_examples 9601 loss: 0.00013828926777932792 %_data_trained : 8.962915935317623
num_of_examples 10001 loss: 0.00015319913043640554 %_data_trained : 9.336370765955857
num_of_examples 10401 loss: 0.00012867964891484008 %_data_trained : 9.709825596594092
num_of_examples 10801 loss: 0.00013510242890333758 %_data_trained : 10.083280427232326
num_of_examples 11201 loss: 0.00013545039284508674 %_data_trained : 10.45673525787056
num_of_examples 11601 loss: 0.00015249993593897672 %_data_trained : 10.830190088508795
num_of_examples 12001 loss: 0.00013551181997172534 %_data_trained : 11.20364491914703
num_of_examples 12401 loss: 0.00013076086179353297 %_data_trained : 11.577099749785264
num_of_examples 12801 loss: 0.00012802816345356405 %_data_trained : 11.950554580423498
num_of_examples 13201 loss: 0.0001530131143226754 %_data_trained : 12.32400941106173
num_of_examples 13601 loss: 0.00012281128671020269 %_data_trained : 12.697464241699965
num_of_examples 14001 loss: 0.0001390746803372167 %_data_trained : 13.0709190723382
num_of_examples 14401 loss: 0.00012300893751671538 %_data_trained : 13.444373902976434
num_of_examples 14801 loss: 0.00014563475298928096 %_data_trained : 13.81782873361467
num_of_examples 15201 loss: 0.00012834111985284835 %_data_trained : 14.191283564252904
num_of_examples 15601 loss: 0.00011430319398641586 %_data_trained : 14.564738394891139
num_of_examples 16001 loss: 0.00012178937889984808 %_data_trained : 14.938193225529373
num_of_examples 16401 loss: 0.00012471398687921464 %_data_trained : 15.311648056167607
num_of_examples 16801 loss: 0.0001355723112646956 %_data_trained : 15.68510288680584
num_of_examples 17201 loss: 0.00013607409644464497 %_data_trained : 16.058557717444074
num_of_examples 17601 loss: 0.0001227625687897671 %_data_trained : 16.43201254808231
num_of_examples 18001 loss: 0.0001142890022310894 %_data_trained : 16.805467378720543
num_of_examples 18401 loss: 0.0001380942815740127 %_data_trained : 17.178922209358777
num_of_examples 18801 loss: 0.00013811752985930071 %_data_trained : 17.552377039997012
num_of_examples 19201 loss: 0.0001020842655270826 %_data_trained : 17.925831870635246
num_of_examples 19601 loss: 0.00011367414757842199 %_data_trained : 18.29928670127348
num_of_examples 20001 loss: 0.0001106110995169729 %_data_trained : 18.672741531911715
num_of_examples 20401 loss: 0.00013049852976109832 %_data_trained : 19.04619636254995
num_of_examples 20801 loss: 0.00010446400076034478 %_data_trained : 19.419651193188184
num_of_examples 21201 loss: 0.00010673243828932754 %_data_trained : 19.793106023826418
num_of_examples 21601 loss: 0.00011096230999100953 %_data_trained : 20.166560854464652
num_of_examples 22001 loss: 0.00011793029931141064 %_data_trained : 20.540015685102887
num_of_examples 22401 loss: 0.00011435791529947891 %_data_trained : 20.91347051574112
num_of_examples 22801 loss: 0.0001114079647231847 %_data_trained : 21.286925346379356
num_of_examples 23201 loss: 0.00011520460146130062 %_data_trained : 21.66038017701759
num_of_examples 23601 loss: 0.0001019034840282984 %_data_trained : 22.033835007655824
num_of_examples 24001 loss: 0.00010651184085872955 %_data_trained : 22.40728983829406
num_of_examples 24401 loss: 9.36361716594547e-05 %_data_trained : 22.780744668932293
num_of_examples 24801 loss: 0.00011615011957474054 %_data_trained : 23.154199499570527
num_of_examples 25201 loss: 9.645152400480583e-05 %_data_trained : 23.52765433020876
num_of_examples 25601 loss: 9.788677140022628e-05 %_data_trained : 23.901109160846996
num_of_examples 26001 loss: 9.821304716751911e-05 %_data_trained : 24.27456399148523
num_of_examples 26401 loss: 0.00010680377694370691 %_data_trained : 24.64801882212346
num_of_examples 26801 loss: 0.00013059734570560977 %_data_trained : 25.0214736527617
num_of_examples 27201 loss: 0.00010948709808872081 %_data_trained : 25.39492848339993
num_of_examples 27601 loss: 9.745899442350491e-05 %_data_trained : 25.768383314038168
num_of_examples 28001 loss: 0.00010993900141329505 %_data_trained : 26.1418381446764
num_of_examples 28401 loss: 0.00010358192696003244 %_data_trained : 26.515292975314637
num_of_examples 28801 loss: 0.00011111733809229918 %_data_trained : 26.888747805952868
num_of_examples 29201 loss: 9.522724358248524e-05 %_data_trained : 27.262202636591105
num_of_examples 29601 loss: 0.00011122738957055844 %_data_trained : 27.63565746722934
num_of_examples 30001 loss: 9.236451835022308e-05 %_data_trained : 28.00911229786757
num_of_examples 30401 loss: 9.74632246652618e-05 %_data_trained : 28.38256712850581
num_of_examples 30801 loss: 8.625820635643322e-05 %_data_trained : 28.75602195914404
num_of_examples 31201 loss: 0.00010403869178844616 %_data_trained : 29.129476789782277
num_of_examples 31601 loss: 9.436638021725229e-05 %_data_trained : 29.502931620420508
num_of_examples 32001 loss: 0.00011319567798636853 %_data_trained : 29.876386451058746
num_of_examples 32401 loss: 9.884198982035742e-05 %_data_trained : 30.249841281696977
num_of_examples 32801 loss: 9.539142367430031e-05 %_data_trained : 30.623296112335215
num_of_examples 33201 loss: 7.511991163482889e-05 %_data_trained : 30.99675094297345
num_of_examples 33601 loss: 8.998007673653774e-05 %_data_trained : 31.37020577361168
num_of_examples 34001 loss: 9.46638232562691e-05 %_data_trained : 31.743660604249918
num_of_examples 34401 loss: 8.88436459354125e-05 %_data_trained : 32.11711543488815
num_of_examples 34801 loss: 9.686913515906781e-05 %_data_trained : 32.49057026552639
num_of_examples 35201 loss: 0.00012139751313952729 %_data_trained : 32.86402509616462
num_of_examples 35601 loss: 0.00010720317557570524 %_data_trained : 33.237479926802855
num_of_examples 36001 loss: 0.00010471549685462379 %_data_trained : 33.610934757441086
num_of_examples 36401 loss: 8.834970401949249e-05 %_data_trained : 33.984389588079324
num_of_examples 36801 loss: 0.00010339885193388909 %_data_trained : 34.357844418717555
num_of_examples 37201 loss: 9.816815618250985e-05 %_data_trained : 34.73129924935579
num_of_examples 37601 loss: 9.22650507709477e-05 %_data_trained : 35.104754079994024
num_of_examples 38001 loss: 8.439049153821543e-05 %_data_trained : 35.47820891063226
num_of_examples 38401 loss: 9.599443175829947e-05 %_data_trained : 35.85166374127049
num_of_examples 38801 loss: 8.432152273599058e-05 %_data_trained : 36.22511857190872
num_of_examples 39201 loss: 9.79979315889068e-05 %_data_trained : 36.59857340254696
num_of_examples 39601 loss: 9.440972811717075e-05 %_data_trained : 36.97202823318519
num_of_examples 40001 loss: 9.590201327227987e-05 %_data_trained : 37.34548306382343
num_of_examples 40401 loss: 8.236959103669506e-05 %_data_trained : 37.71893789446166
num_of_examples 40801 loss: 7.678030204260721e-05 %_data_trained : 38.0923927250999
num_of_examples 41201 loss: 7.029291045910214e-05 %_data_trained : 38.46584755573813
num_of_examples 41601 loss: 7.636518857907504e-05 %_data_trained : 38.83930238637637
num_of_examples 42001 loss: 8.388096423004754e-05 %_data_trained : 39.212757217014605
num_of_examples 42401 loss: 8.876716205122648e-05 %_data_trained : 39.586212047652836
num_of_examples 42801 loss: 7.981426329934038e-05 %_data_trained : 39.959666878291074
num_of_examples 43201 loss: 8.918986626667902e-05 %_data_trained : 40.333121708929305
num_of_examples 43601 loss: 9.54565903521143e-05 %_data_trained : 40.70657653956754
num_of_examples 44001 loss: 8.371976437047124e-05 %_data_trained : 41.080031370205774
num_of_examples 44401 loss: 6.848708482721122e-05 %_data_trained : 41.45348620084401
num_of_examples 44801 loss: 8.061356464168057e-05 %_data_trained : 41.82694103148224
num_of_examples 45201 loss: 8.280660615127999e-05 %_data_trained : 42.20039586212047
num_of_examples 45601 loss: 8.622025889053475e-05 %_data_trained : 42.57385069275871
num_of_examples 46001 loss: 8.126541521050967e-05 %_data_trained : 42.94730552339694
num_of_examples 46401 loss: 7.565726627944969e-05 %_data_trained : 43.32076035403518
num_of_examples 46801 loss: 8.509179475368001e-05 %_data_trained : 43.69421518467341
num_of_examples 47201 loss: 7.299192162463442e-05 %_data_trained : 44.06767001531165
num_of_examples 47601 loss: 6.086174282245338e-05 %_data_trained : 44.44112484594988
num_of_examples 48001 loss: 6.82993855298264e-05 %_data_trained : 44.81457967658812
num_of_examples 48401 loss: 6.35231108753942e-05 %_data_trained : 45.18803450722635
num_of_examples 48801 loss: 7.493847922887653e-05 %_data_trained : 45.561489337864586
num_of_examples 49201 loss: 7.605767772474792e-05 %_data_trained : 45.93494416850282
num_of_examples 49601 loss: 7.750639793812298e-05 %_data_trained : 46.308398999141055
num_of_examples 50001 loss: 8.060564563493244e-05 %_data_trained : 46.68185382977929
num_of_examples 50401 loss: 6.162931520520943e-05 %_data_trained : 47.05530866041752
num_of_examples 50801 loss: 6.616156177187805e-05 %_data_trained : 47.42876349105576
num_of_examples 51201 loss: 6.83118998131249e-05 %_data_trained : 47.80221832169399
num_of_examples 51601 loss: 8.072436903603376e-05 %_data_trained : 48.17567315233223
num_of_examples 52001 loss: 7.374762724793982e-05 %_data_trained : 48.54912798297046
num_of_examples 52401 loss: 6.249609199585393e-05 %_data_trained : 48.92258281360869
num_of_examples 52801 loss: 6.109227921115234e-05 %_data_trained : 49.29603764424692
num_of_examples 53201 loss: 6.743976402503904e-05 %_data_trained : 49.66949247488516
num_of_examples 53601 loss: 6.203511831699871e-05 %_data_trained : 50.0429473055234
num_of_examples 54001 loss: 6.482022468844662e-05 %_data_trained : 50.41640213616163
num_of_examples 54401 loss: 7.52865966205718e-05 %_data_trained : 50.78985696679986
num_of_examples 54801 loss: 7.135306688724086e-05 %_data_trained : 51.163311797438105
num_of_examples 55201 loss: 6.825544784078375e-05 %_data_trained : 51.536766628076336
num_of_examples 55601 loss: 5.3424892357725184e-05 %_data_trained : 51.91022145871457
num_of_examples 56001 loss: 6.548793608089909e-05 %_data_trained : 52.2836762893528
num_of_examples 56401 loss: 5.450620534247719e-05 %_data_trained : 52.65713111999104
num_of_examples 56801 loss: 7.462355904863216e-05 %_data_trained : 53.03058595062927
num_of_examples 57201 loss: 7.287273889232893e-05 %_data_trained : 53.404040781267504
num_of_examples 57601 loss: 6.0111686543677934e-05 %_data_trained : 53.777495611905735
num_of_examples 58001 loss: 6.419682176783681e-05 %_data_trained : 54.15095044254398
num_of_examples 58401 loss: 6.146800456917845e-05 %_data_trained : 54.52440527318221
num_of_examples 58801 loss: 6.345464229525533e-05 %_data_trained : 54.89786010382044
num_of_examples 59201 loss: 7.033509624307044e-05 %_data_trained : 55.27131493445868
num_of_examples 59601 loss: 5.5939481680979954e-05 %_data_trained : 55.64476976509691
num_of_examples 60001 loss: 6.06882923602825e-05 %_data_trained : 56.01822459573514
num_of_examples 60401 loss: 6.254288884520065e-05 %_data_trained : 56.39167942637337
num_of_examples 60801 loss: 5.279773598886095e-05 %_data_trained : 56.76513425701162
num_of_examples 61201 loss: 6.434313734644093e-05 %_data_trained : 57.13858908764985
num_of_examples 61601 loss: 4.943002375512151e-05 %_data_trained : 57.51204391828808
num_of_examples 62001 loss: 5.921266769291833e-05 %_data_trained : 57.885498748926324
num_of_examples 62401 loss: 4.962277580489172e-05 %_data_trained : 58.258953579564555
num_of_examples 62801 loss: 5.898906609218102e-05 %_data_trained : 58.632408410202785
num_of_examples 63201 loss: 5.570546090893913e-05 %_data_trained : 59.005863240841016
num_of_examples 63601 loss: 5.441915738629177e-05 %_data_trained : 59.37931807147926
num_of_examples 64001 loss: 6.120644611655735e-05 %_data_trained : 59.75277290211749
num_of_examples 64401 loss: 6.110298592830076e-05 %_data_trained : 60.12622773275572
num_of_examples 64801 loss: 5.61924103385536e-05 %_data_trained : 60.499682563393954
num_of_examples 65201 loss: 5.206816364079714e-05 %_data_trained : 60.8731373940322
num_of_examples 65601 loss: 5.491042149515124e-05 %_data_trained : 61.24659222467043
num_of_examples 66001 loss: 5.764464458479779e-05 %_data_trained : 61.62004705530866
num_of_examples 66401 loss: 5.1438243099255486e-05 %_data_trained : 61.9935018859469
num_of_examples 66801 loss: 6.0281396727077663e-05 %_data_trained : 62.36695671658513
num_of_examples 67201 loss: 4.3552846982493065e-05 %_data_trained : 62.74041154722336
num_of_examples 67601 loss: 5.262239275907632e-05 %_data_trained : 63.11386637786159
num_of_examples 68001 loss: 4.878809359070147e-05 %_data_trained : 63.487321208499836
num_of_examples 68401 loss: 5.449055832286831e-05 %_data_trained : 63.86077603913807
num_of_examples 68801 loss: 4.4419426048989406e-05 %_data_trained : 64.2342308697763
num_of_examples 69201 loss: 5.3734947578050196e-05 %_data_trained : 64.60768570041454
num_of_examples 69601 loss: 4.268577140464913e-05 %_data_trained : 64.98114053105277
num_of_examples 70001 loss: 4.461745669686934e-05 %_data_trained : 65.354595361691
num_of_examples 70401 loss: 5.52564026293112e-05 %_data_trained : 65.72805019232923
num_of_examples 70801 loss: 4.9141309791593815e-05 %_data_trained : 66.10150502296747
num_of_examples 71201 loss: 5.6885309604695066e-05 %_data_trained : 66.47495985360571
num_of_examples 71601 loss: 5.440037464722991e-05 %_data_trained : 66.84841468424393
num_of_examples 72001 loss: 4.790536804648582e-05 %_data_trained : 67.22186951488217
num_of_examples 72401 loss: 4.525024531176314e-05 %_data_trained : 67.59532434552041
num_of_examples 72801 loss: 4.545555129880086e-05 %_data_trained : 67.96877917615865
num_of_examples 73201 loss: 6.094940756156575e-05 %_data_trained : 68.34223400679687
num_of_examples 73601 loss: 5.638160517264623e-05 %_data_trained : 68.71568883743511
num_of_examples 74001 loss: 4.8981983127305284e-05 %_data_trained : 69.08914366807335
num_of_examples 74401 loss: 4.733588648377918e-05 %_data_trained : 69.46259849871159
num_of_examples 74801 loss: 4.8281006893375885e-05 %_data_trained : 69.83605332934981
num_of_examples 75201 loss: 5.7866034840117205e-05 %_data_trained : 70.20950815998805
num_of_examples 75601 loss: 5.803293846838642e-05 %_data_trained : 70.58296299062629
num_of_examples 76001 loss: 4.2300137138227e-05 %_data_trained : 70.95641782126452
num_of_examples 76401 loss: 4.917184342048131e-05 %_data_trained : 71.32987265190275
num_of_examples 76801 loss: 5.465616693982156e-05 %_data_trained : 71.70332748254098
num_of_examples 77201 loss: 4.538353481621016e-05 %_data_trained : 72.07678231317922
num_of_examples 77601 loss: 5.724635455408133e-05 %_data_trained : 72.45023714381745
num_of_examples 78001 loss: 5.7151801884174346e-05 %_data_trained : 72.82369197445568
num_of_examples 78401 loss: 3.99682144052349e-05 %_data_trained : 73.19714680509392
num_of_examples 78801 loss: 4.990157831343822e-05 %_data_trained : 73.57060163573216
num_of_examples 79201 loss: 4.627434376743622e-05 %_data_trained : 73.94405646637038
Traceback (most recent call last):
  File "/home/stanlo229/Research/Repos/MolFoundation/t5-chem/run_script.py", line 167, in <module>
    train_one_epoch(epoch)
  File "/home/stanlo229/Research/Repos/MolFoundation/t5-chem/run_script.py", line 88, in train_one_epoch
    total_loss += nn_loss.item()
                  ^^^^^^^^^^^^^^
KeyboardInterrupt