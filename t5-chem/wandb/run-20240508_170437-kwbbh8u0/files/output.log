/home/stanlo229/anaconda3/envs/MolFoundation/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/home/stanlo229/anaconda3/envs/MolFoundation/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Epoch 0
num_of_examples 1 loss: 0.0003574422001838684 %_data_trained : 0.0
/home/stanlo229/Research/Repos/MolFoundation/t5-chem/data_utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  "y_regression_values": torch.tensor(self.y_regression_values[idx]).to(self.device),
num_of_examples 401 loss: 0.007623336091637611 %_data_trained : 5.0
num_of_examples 801 loss: 0.005767894629389048 %_data_trained : 10.0
num_of_examples 1201 loss: 0.004605607334524393 %_data_trained : 15.0
num_of_examples 1601 loss: 0.003207816015928984 %_data_trained : 20.0
num_of_examples 2001 loss: 0.0024497877387329938 %_data_trained : 25.0
num_of_examples 2401 loss: 0.00222663979511708 %_data_trained : 30.0
num_of_examples 2801 loss: 0.0016768498811870814 %_data_trained : 35.0
num_of_examples 3201 loss: 0.0017542263981886208 %_data_trained : 40.0
num_of_examples 3601 loss: 0.001392992112087086 %_data_trained : 45.0
num_of_examples 4001 loss: 0.001155728460289538 %_data_trained : 50.0
num_of_examples 4401 loss: 0.0009202557825483382 %_data_trained : 55.00000000000001
num_of_examples 4801 loss: 0.0007221520901657641 %_data_trained : 60.0
num_of_examples 5201 loss: 0.0005470896291080862 %_data_trained : 65.0
num_of_examples 5601 loss: 0.0005406820902135223 %_data_trained : 70.0
num_of_examples 6001 loss: 0.000404215493472293 %_data_trained : 75.0
num_of_examples 6401 loss: 0.00026547094341367484 %_data_trained : 80.0
num_of_examples 6801 loss: 0.00024144873634213582 %_data_trained : 85.0
num_of_examples 7201 loss: 0.00018088394339429214 %_data_trained : 90.0
num_of_examples 7601 loss: 0.00016530235152458772 %_data_trained : 95.0